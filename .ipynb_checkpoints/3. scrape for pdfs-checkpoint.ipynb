{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import config\n",
    "from word2number import w2n\n",
    "from selenium import webdriver\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import TimeoutError                                  \n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "# for browser interaction & load waiting:\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_nohidden(path):\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns target element once it is loaded\n",
    "def wait_for(element, by, timeout=10, multi=False):\n",
    "    global d\n",
    "    time.sleep(.25)\n",
    "    try:\n",
    "        WebDriverWait(d, timeout).until(\n",
    "            EC.presence_of_element_located((by, element)))\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout limit reached ({timeout} s)\")\n",
    "        print(f\"> Seaching for {element} by: {by}\")\n",
    "    finally:\n",
    "        time.sleep(.25)\n",
    "        if multi: return d.find_elements(by, element)\n",
    "        else: return d.find_element(by, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_new_search(verb=True):\n",
    "    global d\n",
    "    # open search panel\n",
    "    url = 'https://scerisecm.boston.gov/ScerIS/CmPublic/#/Home'\n",
    "    open_search = '/html/body/div[1]/div/div/div[2]/div[3]/search-dashboard/div/div/div[2]/div/div/div/div/ul/li[3]/dashboard-folders-renderer/div/div/div/a[3]'\n",
    "    if d: d.quit()\n",
    "   \n",
    "    d = webdriver.Chrome()\n",
    "    d.set_window_size(1000,800)\n",
    "    d.set_window_position(0,0)\n",
    "\n",
    "    d.get(url)\n",
    "    time.sleep(.5)\n",
    "    wait_for(open_search, By.XPATH).click()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_params(verb=True):\n",
    "    # define search params and their corresponding input elements on the page\n",
    "    params = wait_for(\"div[ng-repeat='dynamicTerm in dynamicSearchFieldTerms'\",\n",
    "                      By.CSS_SELECTOR, multi=True)\n",
    "    names = [p.text.split('\\n')[0] for p in params]\n",
    "    input_containers = wait_for('input-with-help', By.CLASS_NAME, multi=True)\n",
    "    inputs = [i.find_element_by_tag_name('input') for i in input_containers]\n",
    "    \n",
    "    # list available params\n",
    "    search_param = {n: {'title':p,'input':i}  for n, p, i in zip(names, params, inputs)}\n",
    "    \n",
    "    return(search_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_search(param, keys, verb=True):\n",
    "    global search_param\n",
    "    param = search_param[param]\n",
    "    if verb:\n",
    "        d.execute_script(\"arguments[0].scrollIntoView();\", param['title'])\n",
    "    param['input'].send_keys(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(adco_adrs, adrs, permit, n, n_items, year,\n",
    "                  download_dir='../../Downloads', target_dir='permit_pdfs/'):\n",
    "    time.sleep(.5)  \n",
    "\n",
    "    def drop_ext(fn):\n",
    "        return ''.join(fn.split('.')[:-1])\n",
    "\n",
    "    # target file name \n",
    "    info = year+' - '+str(permit)+'-'+str(n)\n",
    "    \n",
    "    # if file already exists, cancel download\n",
    "    \n",
    "    pre_down = [drop_ext(p) for p in listdir_nohidden(target_dir+adco_adrs+'/'+adrs)]\n",
    "    if any(info in p for p in pre_down):\n",
    "        print(f'    |-[{n+1}/{n_items}] Already downloaded: {info} ')\n",
    "        return\n",
    "        \n",
    "    \n",
    "    print(f'    |-[{n+1}/{n_items}] Downloading {info}...')\n",
    "\n",
    "    item = d.find_elements(By.CLASS_NAME, 'ui-grid-row')[n].click()\n",
    "    wait_for(\"button[ng-click='openDocuments()']\", By.CSS_SELECTOR).click()\n",
    "    wait_for(\"button[data-pcc-toggle='dialog-download']\", By.CSS_SELECTOR).click()\n",
    "    wait_for(\"button[data-pcc-download='download']\", By.CSS_SELECTOR).click()\n",
    "    pages = int(d.find_element(By.CSS_SELECTOR, 'span[data-pcc-pagecount]').text)\n",
    "    \n",
    "    wait_for(\"pcc-overlay-download\", By.CLASS_NAME).click()\n",
    "    \n",
    "    # if big document, download first ten pages only\n",
    "    if pages > 10:\n",
    "        print('      |-Large file - splicing first 10 pages...')\n",
    "        n = str(n)+'_first10pg'\n",
    "        wait_for(\"rdoRangeCustom\", By.ID).click()\n",
    "        wait_for(\"txtCustomRange\", By.ID).send_keys('1-10')\n",
    "    wait_for(\"OK\", By.ID).click()\n",
    "    \n",
    "    \n",
    "    # target current download in the downloads folder\n",
    "    print(f\"      |-Downloading '{filename}'...\")\n",
    "    filename = max([f for f in os.listdir(download_dir)],\n",
    "                   key=lambda xa : os.path.getctime(os.path.join(download_dir,xa)))\n",
    "\n",
    "    # monitor file for download completion\n",
    "    waits = 0\n",
    "    while '.part' in filename or 'crdownload' in filename: # wait while downloading\n",
    "        filename = max([f for f in os.listdir(download_dir)],\n",
    "                   key=lambda xa : os.path.getctime(os.path.join(download_dir,xa)))\n",
    "                   # credit to dmb for targeting download file\n",
    "                   # stackoverflow.com/questions/34548041\n",
    "        waits +=1\n",
    "        time.sleep(.33)\n",
    "        print(f\"      |-Downloading '{info}'...\")\n",
    "        if waits>30: # 10 seconds\n",
    "            raise TimeoutError(\"error:|-Download timed out!\")\n",
    "    \n",
    "    # rename new file\n",
    "    newname = info+'.'+filename.split('.')[-1] # add original extension\n",
    "    os.rename(os.path.join(download_dir, filename), os.path.join(download_dir, newname))\n",
    "    print(f\"      |-File renamed to '{filename}'...\")\n",
    "\n",
    "    # move file to this directory\n",
    "    time.sleep(.2)\n",
    "    shutil.move(os.path.join(download_dir, newname), target_dir+adco_adrs+'/'+adrs+'/'+newname)\n",
    "    print(f\"      |-File moved: '{newname}'\")\n",
    "\n",
    "    if o_len == os.listdir(target_dir+adco_adrs+'/'+adrs):\n",
    "        raise TimeoutError('error:|-File not found after download!')\n",
    "    print(f\"      |-File saved: '{newname}'\")\n",
    "    \n",
    "    wait_for(\"btnBackToSearchResult\", By.ID).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_building_docs(row, target_dir='permit_pdfs/'):\n",
    "    print('—'*30)\n",
    "    adco_adrs = str(row['GPSaddress'])\n",
    "    st_num = row['Primary Street Number']\n",
    "    st_name = row['Street Name']\n",
    "    adrs = f'{st_num}_{st_name}'\n",
    "    permit = row['Permit Number']\n",
    "    year = row['Permit Date'][-4:]\n",
    "    \n",
    "    # launch fresh search\n",
    "    print('  |-Launching new search...')\n",
    "    try: launch_new_search()\n",
    "    except:\n",
    "        if d: d.quit()\n",
    "        d = False\n",
    "        launch_new_search()\n",
    "    \n",
    "    # set search params\n",
    "    global search_param\n",
    "    print('  |-Defining search parameters...')\n",
    "    search_param = get_search_params()\n",
    "    set_search('Document Type', 'C/O')\n",
    "    params = ['Primary Street Number', 'Street Name', 'Permit Number']\n",
    "    for param in params:\n",
    "        set_search(param, str(row[param]))\n",
    "        time.sleep(.25)\n",
    "    # get search results\n",
    "    print('  |-Getting results...')\n",
    "    time.sleep(.5)\n",
    "    wait_for('btnSearch', By.ID).click()\n",
    "    \n",
    "    # create target download directory (to move file to)\n",
    "    if not os.path.exists(target_dir+adco_adrs+'/'+adrs):\n",
    "        os.makedirs(target_dir+adco_adrs+'/'+adrs)\n",
    "        print('    |-Creating parent directory...')\n",
    "    else: print('    |-Found parent directory...')\n",
    "\n",
    "    # check pre-existing files\n",
    "    o_len = len(os.listdir(target_dir+adco_adrs+'/'+adrs))\n",
    "    print('    |-'+target_dir+adco_adrs+'/'+adrs+'/')\n",
    "    print(f'      |-{o_len} existing files found')\n",
    "    \n",
    "    downloaded  = 0 # for logging \n",
    "    time.sleep(.5)\n",
    "    time.sleep(1)\n",
    "    items = wait_for('ui-grid-row', By.CLASS_NAME, multi=True)\n",
    "    n_items = len(items)\n",
    "\n",
    "    for n in range(n_items):\n",
    "        download_file(adco_adrs, adrs, permit, n, n_items, year)\n",
    "        downloaded +=1 \n",
    "        \n",
    "    print(f'  |-Done - {downloaded} files downloaded')\n",
    "    print('—'*60)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_scraper():\n",
    "    \n",
    "    start = time.gmtime()\n",
    "    globals()['to_search'] = pd.read_csv('sceris_adco_merged.csv')\n",
    "    global to_search\n",
    "    globals()['d'] = False\n",
    "    global d\n",
    "    max_rdx_tries = 3\n",
    "    completed = 0\n",
    "    \n",
    "    to_search = to_search.drop_duplicates(\n",
    "        subset=['Primary Street Number', 'Street Name', 'Permit Number'])\n",
    "    \n",
    "    \n",
    "    deepest_idx = to_search[to_search['Permit Number']=='COO712195'].index[0]\n",
    "    to_search = to_search.loc[deepest_idx+1:]\n",
    "    \n",
    "    for ridx in tqdm(to_search.index):\n",
    "        row = to_search.loc[ridx]\n",
    "        st_num = row['Primary Street Number']\n",
    "        st_name = row['Street Name']\n",
    "        adco_adrs = row['GPSaddress']\n",
    "        permit = row['Permit Number']\n",
    "\n",
    "        adrs = str(st_num)+'_'+st_name\n",
    "        print('> Acquiring C/Os for', adrs.strip(), '-', permit.strip())\n",
    "        wait_between = 2\n",
    "        ridx_tries = 0\n",
    "        success = False\n",
    "        while True:\n",
    "            ridx_tries+=1\n",
    "            if ridx_tries > max_rdx_tries: break\n",
    "            try:\n",
    "                scrape_building_docs(row)\n",
    "                success = True\n",
    "                time.sleep(.2)\n",
    "                break\n",
    "            except:\n",
    "                print(f'>>> Failed! Retrying... (Try {ridx_tries}/{max_rdx_tries})')\n",
    "                d.quit()\n",
    "                d = False\n",
    "                time.sleep(wait_between)\n",
    "                wait_between **= 2\n",
    "        if not success:\n",
    "            print('*'*5, 'ERROR: FAILED on ridx:', ridx)\n",
    "            try: d.quit()\n",
    "            except: pass\n",
    "            print('ERROR DETAILS: > Building ID:', row['Building ID'])\n",
    "        else:\n",
    "            completed += 1\n",
    "            \n",
    "    print(f'\\n>>> Completed downloading C/Os for{completed}/{len(to_search)} rows from target data.')\n",
    "    print(f'\\n  > Time elapsed: {time.strftime(time.gmtime()-start)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d8a52ba514cddb9d0c9a17ba8fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Acquiring C/Os for 103_ARCH - 46020\n",
      "——————————————————————————————\n",
      "  |-Launching new search...\n",
      "  |-Defining search parameters...\n",
      "  |-Getting results...\n",
      "    |-Found parent directory...\n",
      "    |-permit_pdfs/103 Arch Street, Boston MA 02110/103_ARCH                                    /\n",
      "      |-0 existing files found\n",
      "    |-[1/1] Downloading 2002 - 46020-0...\n",
      ">>> Failed! Retrying... (Try 1/3)\n",
      "——————————————————————————————\n",
      "  |-Launching new search...\n",
      "  |-Defining search parameters...\n",
      "  |-Getting results...\n",
      "    |-Found parent directory...\n",
      "    |-permit_pdfs/103 Arch Street, Boston MA 02110/103_ARCH                                    /\n",
      "      |-0 existing files found\n",
      "    |-[1/1] Downloading 2002 - 46020-0...\n",
      ">>> Failed! Retrying... (Try 2/3)\n",
      "——————————————————————————————\n",
      "  |-Launching new search...\n",
      ">>> Failed! Retrying... (Try 3/3)\n",
      "***** ERROR: FAILED on ridx: 78\n",
      "ERROR DETAILS: > Building ID: 0\n",
      "> Acquiring C/Os for 103_ARCH - 632292\n",
      "——————————————————————————————\n",
      "  |-Launching new search...\n",
      ">>> Failed! Retrying... (Try 1/3)\n",
      "——————————————————————————————\n",
      "  |-Launching new search...\n",
      "  |-Defining search parameters...\n",
      "  |-Getting results...\n",
      "    |-Found parent directory...\n",
      "    |-permit_pdfs/103 Arch Street, Boston MA 02110/103_ARCH                                    /\n",
      "      |-0 existing files found\n",
      "    |-[1/1] Downloading 2016 - 632292-0...\n"
     ]
    }
   ],
   "source": [
    "launch_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
